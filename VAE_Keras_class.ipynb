{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "# Parameters \n",
    "\n",
    "img_shape = (28, 28, 1)\n",
    "batch_size = 150\n",
    "latent_dim = 2\n",
    "epochs = 30\n",
    "optimizer = 'adam'\n",
    "\n",
    "class VAE(keras.layers.Layer):\n",
    "    \n",
    "    import keras\n",
    "    from keras import layers\n",
    "    from keras.models import Model\n",
    "    from keras import backend as K\n",
    "    \n",
    "    def __init__(self, img_shape, latent_dim, batch_size, epochs, optimizer, x_train, x_test):\n",
    "        self.img_shape = img_shape\n",
    "        self.latent_dim = latent_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.x_train = x_train\n",
    "        self.x_test = x_test\n",
    "        self.input_layer = layers.Input(shape=self.img_shape)\n",
    "    \n",
    "    # gaussian encoder\n",
    "    def model(self):\n",
    "#         i = layers.Input(shape=self.img_shape)\n",
    "        i = self.input_layer\n",
    "        # Convolution layer \n",
    "        c = layers.Conv2D(32, 3, padding='same', activation='relu')(i)\n",
    "        c = layers.Conv2D(64, 3, padding='same', activation='relu', strides=(2, 2))(c)\n",
    "        c = layers.Conv2D(64, 3, padding='same', activation='relu')(c)\n",
    "        e = layers.Conv2D(64, 3, padding='same', activation='relu')(c)\n",
    "        # shape before_Dense\n",
    "        self.shape_before_dense = K.int_shape(e)\n",
    "        # \n",
    "        x = layers.Flatten()(e)\n",
    "        x = layers.Dense(32, activation='relu')(x)\n",
    "        \n",
    "        # q_pie parameters \n",
    "        self.z_mean = layers.Dense(self.latent_dim)(x)\n",
    "        self.z_log_var = layers.Dense(self.latent_dim)(x)\n",
    "        # sampling\n",
    "        \n",
    "        ez = layers.Lambda(self.sampling)([self.z_mean, self.z_log_var])\n",
    "        \n",
    "        z = layers.Input(K.int_shape(ez)[1:])\n",
    "        x = layers.Dense(32)(z)\n",
    "        x = layers.Dense(np.prod(self.shape_before_dense[1:]), activation='relu')(z)\n",
    "        x = layers.Reshape(self.shape_before_dense[1:])(x)\n",
    "        x = layers.Conv2DTranspose(32, 3, padding='same', activation='relu', strides=(2, 2))(x)\n",
    "        z_decoded = layers.Conv2D(1, 3, padding='same', activation='sigmoid')(x)\n",
    "        \n",
    "        return z_decoded\n",
    "    \n",
    "#     # bernoulli decoder\n",
    "#     def decoder(self):\n",
    "#         i = layers.Input(shape=self.img_shape)\n",
    "#         z = self.encoder()(i)\n",
    "#         z = layers.Input(K.int_shape(z))\n",
    "#         x = layers.Dense(32)(z)\n",
    "#         x = layers.Dense(np.prod(self.shape_before_dense), activation='relu')(z)\n",
    "#         x = layers.Reshape(self.shape_before_dense)(x)\n",
    "#         x = layers.Conv2DTranspose(32, 3, padding='same', activation='relu', stride=(2, 2))(x)\n",
    "#         reconstruction = layers.Conv2D(1, 3, padding='same', activation='sigmoid')(x)\n",
    "        \n",
    "#         return reconstruction\n",
    "    \n",
    "    def sampling(self, args):\n",
    "        \"\"\"\n",
    "        q_pie(Z)에서 샘플링을 한 것을 Decoder(Generator)로 넘기기 때문에, 원칙적으로는 Back-propagation이 안된다. 이를 가능하게 하기 위해서 \n",
    "        사용한 트릭으로, 아래와 같은 수식으로 샘플링을 하면, 원 확률분포의 특성을 해치지 않으면서, back-propagation이 가능하다. \n",
    "        \"\"\"\n",
    "        z_mean, z_log_var = args\n",
    "                                            # batch     # batch \n",
    "        ep = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0, stddev=1)\n",
    "        \n",
    "        return z_mean + K.exp(z_log_var) * ep\n",
    "    \n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        xent_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
    "\n",
    "        k1_loss = -5e-4 * K.mean(\n",
    "            1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + k1_loss)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, z_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return x\n",
    "    \n",
    "    def train(self):\n",
    "#         z_decoded = self.decoder(self.encoder(x))\n",
    "        x = self.input_layer\n",
    "        z_decoded = self.model()\n",
    "        \n",
    "        model = Model(x, z_decoded)\n",
    "        model.compile(optimizer=self.optimizer, loss=None)\n",
    "        model.summary()\n",
    "        \n",
    "#         model.fit(x=self.x_train, y=None, shuffle=True, epochs=self.epochs, \n",
    "#                   batch_size=self.batch_size, validation_data=(self.x_test, None))\n",
    "\n",
    "vae = VAE(img_shape, latent_dim, batch_size, epochs, optimizer, x_train, x_test)\n",
    "vae.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
